{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN7AxEdNyhxKNuIetldmH1Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["First define two classes for the Variational Autoencoder models, `trmVAE1` and `trmVAE2`. `trmVAE1` uses BERT as the encoder, while `trmVAE2` uses a Transformer as both the encoder and decoder. Both models include a reparameterization step in the forward pass, which is a key component of Variational Autoencoders.\n","\n","Next, define a function `train_model` to train the model. This function takes in the model, dataloader, optimizer, scheduler, and number of epochs as arguments. It performs the forward pass, computes the loss, performs the backward pass and optimization, steps the learning rate scheduler, and increases the weight of the KL divergence term.\n","\n","Finally, the `main` function is used to load and preprocess the data, create a DataLoader, initialize the model, create an optimizer, define the learning rate scheduler, and train the model."],"metadata":{"id":"54dEoFkQal1R"}},{"cell_type":"markdown","source":["# Step 3: Build the Model\n","## Solution 3\n","* Transformer encoder + VAE model"],"metadata":{"id":"hO5OagjtaXPp"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","from torch.optim import AdamW\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","from transformers import BertModel, BertConfig, get_linear_schedule_with_warmup\n","import torch.nn.functional as F"],"metadata":{"id":"NyMIiR_watfC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MyDataset(Dataset):\n","    \"\"\"Custom Dataset for loading the 5-dimensional points.\"\"\"\n","\n","    def __init__(self, data):\n","        self.data = data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return self.data[idx]\n","\n","class trmVAE1(nn.Module):\n","    \"\"\"Variational Autoencoder with BERT as encoder.\"\"\"\n","\n","    def __init__(self, latent_dim, output_dim=5):\n","        super(trmVAE1, self).__init__()\n","        # Initialize the BERT model\n","        self.encoder = BertModel(BertConfig())\n","        # Define the linear layers for the mean and log variance\n","        self.fc_mu = nn.Linear(self.encoder.config.hidden_size, latent_dim)\n","        self.fc_var = nn.Linear(self.encoder.config.hidden_size, latent_dim)\n","        # Define the decoder\n","        self.decoder = nn.Sequential(\n","            nn.Linear(latent_dim, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, output_dim)\n","        )\n","\n","    def forward(self, x):\n","        encoded = self.encoder(x)\n","        mu = self.fc_mu(encoded.last_hidden_state)\n","        log_var = self.fc_var(encoded.last_hidden_state)\n","        z = self.reparameterize(mu, log_var)\n","        decoded = self.decoder(z)\n","        return decoded[:, -1, :], mu, log_var  # Only return the last output\n","\n","    def reparameterize(self, mu, log_var):\n","        std = torch.exp(0.5*log_var)\n","        eps = torch.randn_like(std) # Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval 0,1\n","        return mu + eps*std\n","\n","class trmVAE2(nn.Module):\n","    \"\"\"Variational Autoencoder with Transformer as encoder and decoder.\"\"\"\n","\n","    def __init__(self, input_dim=5, latent_dim=5):\n","        super(trmVAE2, self).__init__()\n","\n","        # Define the encoder\n","        encoder_layers = TransformerEncoderLayer(input_dim, 1,dim_feedforward=32) # ninp, nhead, nhid\n","        self.encoder = TransformerEncoder(encoder_layers, 2) # nlayers\n","\n","        # Define the decoder\n","        decoder_layers = TransformerEncoderLayer(latent_dim, 2,dim_feedforward=128,activation='gelu')\n","        self.decoder = TransformerEncoder(decoder_layers, 6)\n","\n","        self.fc_mu = nn.Linear(input_dim, latent_dim)\n","        self.fc_var = nn.Linear(input_dim, latent_dim)\n","\n","        self.fc3 = nn.Linear(latent_dim, input_dim)\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5 * logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps * std\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        mu = self.fc_mu(x)\n","        mu = mu.to(torch.float32)\n","        logvar = self.fc_var(x)\n","        logvar = logvar.to(torch.float32)\n","        z = self.reparameterize(mu, logvar)\n","        return self.fc3(self.decoder(z)), mu, logvar # If different latent space\n"],"metadata":{"id":"8QodKzZgawi2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 4: Train the Model"],"metadata":{"id":"YXNDrqhtbLmx"}},{"cell_type":"code","source":["def train_model(model, dataloader, optimizer, scheduler, epochs=100):\n","    \"\"\"Train the model.\n","\n","    Args:\n","        model (nn.Module): The model to be trained.\n","        dataloader (DataLoader): The data loader.\n","        optimizer (Optimizer): The optimizer.\n","        scheduler (lr_scheduler): The learning rate scheduler.\n","        epochs (int, optional): The number of epochs. Defaults to 100.\n","    \"\"\"\n","    # Initialize the weight of the KL divergence term\n","    kld_weight = 4.0\n","    # Define the rate at which the weight is increased\n","    annealing_rate = 0.001\n","\n","    # Train the model\n","    for epoch in range(epochs):\n","        for batch in dataloader:\n","            # Forward pass\n","            recon_batch, mu, log_var = model(batch.to(\"cuda\"))\n","            recon_batch = recon_batch.to(torch.float32)\n","            batch = batch.to(torch.float32).to(\"cuda\")\n","            # Compute the loss\n","            recon_loss = F.mse_loss(recon_batch, batch)\n","            kld_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n","            loss = recon_loss + kld_loss*kld_weight\n","            # Backward pass and optimization\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            # Step the learning rate scheduler\n","            scheduler.step()\n","            # Increase the weight of the KL divergence term\n","            kld_weight = min(5, kld_weight + annealing_rate)\n","        print(f'Epoch {epoch}, Loss {loss.item()}')"],"metadata":{"id":"8wWhjcyja4Tn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def main():\n","    \"\"\"Main function to build and train the model.\"\"\"\n","    # Load and preprocess the data\n","    data = load_data() # Step 1&2\n","    data = normalize_data(data)\n","\n","    # Create a DataLoader\n","    dataloader = DataLoader(MyDataset(data), batch_size=32)\n","\n","    # Initialize the model\n","    # model = trmVAE1()\n","    model = trmVAE2()\n","\n","    # Create an optimizer\n","    optimizer = AdamW(model.parameters(), lr=1e-3)\n","\n","    # Define the learning rate scheduler\n","    epochs = 100\n","    total_steps = len(dataloader.dataset) * epochs\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(total_steps/10), num_training_steps=total_steps)\n","\n","    # Train the model\n","    train_model(model, dataloader, optimizer, scheduler, epochs)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"UVR3dN5Fa9Sw"},"execution_count":null,"outputs":[]}]}