{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNaMdUyv13PHF/QQuhSV184"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["First build the GPT-2 model using the `build_model` function. This function initializes and returns the GPT-2 model.\n","\n","Next, prepare the data using the `prepare_data` function. This function takes in the dataset, converts the data to sequences, tokenizes the sequences, pads all sequences to the same length, and converts the sequences to PyTorch tensors.\n","\n","Then, train the model using the `train_model` function. This function takes in the model and the sequences, prepares the optimizer, (moves the model to GPU), sets the model to training mode, and trains the model for number of epochs.\n","\n","Finally, the `main` function is used to call these functions. It loads and prepares the data, builds the model, and trains the model."],"metadata":{"id":"54dEoFkQal1R"}},{"cell_type":"markdown","source":["# Step 3: Build the Model\n","## Solution 2\n","* GPT-2 Model"],"metadata":{"id":"hO5OagjtaXPp"}},{"cell_type":"code","source":["from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n","import torch\n","import random"],"metadata":{"id":"NyMIiR_watfC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_model():\n","    \"\"\"Build and return the GPT-2 model.\n","\n","    Returns:\n","        model (GPT2LMHeadModel): The GPT-2 model.\n","    \"\"\"\n","    # Initialize the GPT-2 model\n","    model = GPT2LMHeadModel.from_pretrained('gpt2')\n","\n","    return model\n","\n","def prepare_data(dataset):\n","    \"\"\"Prepare the data for the model.\n","\n","    Args:\n","        dataset (list): The dataset to be prepared.\n","\n","    Returns:\n","        sequences (torch.Tensor): The prepared data.\n","    \"\"\"\n","    # Initialize the tokenizer\n","    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","    # Convert the data to sequences\n","    sequences = []\n","    for i in range(len(dataset)):\n","        # Convert the 5D points to strings and concatenate them into a \"sequence\"\n","        sequence = [str(round(x, 8)).ljust(10, '0') for x in dataset[i]] # Same length float string\n","        sequences.append(\" \".join(sequence))\n","\n","    # Use the tokenizer to convert the \"sequence\" into a sequence of tokens\n","    sequences = [tokenizer.encode(seq + '\\n' + sequences[i+1]) for i, seq in enumerate(sequences[:-1])]\n","\n","    # Set the padding token if it's not already set\n","    if tokenizer.pad_token is None:\n","        tokenizer.pad_token = tokenizer.eos_token\n","\n","    # Pad all sequences to the same length\n","    max_length = max(len(seq) for seq in sequences)\n","    sequences = [seq + [tokenizer.pad_token_id] * (max_length - len(seq)) for seq in sequences]\n","\n","    # Convert the sequences to PyTorch tensors\n","    sequences = torch.tensor(sequences)\n","\n","    return sequences"],"metadata":{"id":"8QodKzZgawi2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 4: Train the Model"],"metadata":{"id":"YXNDrqhtbLmx"}},{"cell_type":"code","source":["def train_model(model, sequences, epochs=100):\n","    \"\"\"Train the model.\n","\n","    Args:\n","        model (GPT2LMHeadModel): The model to be trained.\n","        sequences (torch.Tensor): The training data.\n","        epochs (int, optional): The number of epochs. Defaults to 100.\n","    \"\"\"\n","    # Prepare the optimizer\n","    optimizer = AdamW(model.parameters())\n","\n","    # Move the model to GPU\n","    model.to(\"cuda\")\n","\n","    # Set the model to training mode\n","    model.train(True)\n","\n","    # Train the model\n","    for epoch in range(epochs):\n","        # Shuffle the indices\n","        idx_li = list(range(len(sequences)))\n","        random.shuffle(idx_li)\n","\n","        for i, idx in enumerate(idx_li[:-1]):\n","            # Get the inputs and targets\n","            inputs = targets = sequences[idx].to(\"cuda\")\n","\n","            # Forward pass\n","            optimizer.zero_grad()\n","            outputs = model(inputs, labels=targets)\n","            loss = outputs.loss\n","\n","            # Backward pass and optimize\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Print the loss every 100 steps\n","            if i % 100 == 0:\n","                print(f'Loss at step {i}: {loss.item()}')"],"metadata":{"id":"8wWhjcyja4Tn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def main():\n","    \"\"\"Main function to build and train the model.\"\"\"\n","    # Load and prepare the data\n","    dataset = load_data() # Step 1&2\n","    sequences = prepare_data(dataset)\n","\n","    # Build the model\n","    model = build_model()\n","\n","    # Train the model\n","    train_model(model, sequences)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"UVR3dN5Fa9Sw"},"execution_count":null,"outputs":[]}]}