{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM0m71K+jndZ39rtDraGB7F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["First define two Variational Autoencoder (VAE) models, `VAE1` and `VAE2`. Both models have an encoder and a decoder, and use the reparameterization trick to sample from the latent space. The difference between the two models is the number of hidden layers in the encoder and decoder.\n","\n","Next, initialize the model and create a data loader for the dataset. Also initialize the optimizer and define the loss function, which is a combination of reconstruction loss and KL divergence loss.\n","\n","Finally, train the model for a specified number of epochs, updating the model parameters with each batch of data. The loss for each epoch is printed to monitor the training process."],"metadata":{"id":"54dEoFkQal1R"}},{"cell_type":"markdown","source":["# Step 3: Build the Model\n","## Solution 4\n","* VAE model"],"metadata":{"id":"hO5OagjtaXPp"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.nn import functional as F\n","from torch.utils.data import DataLoader, TensorDataset"],"metadata":{"id":"NyMIiR_watfC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert the dataset to a PyTorch DataLoader\n","data_loader = torch.utils.data.DataLoader(data, batch_size=32, shuffle=True)\n","latent_dim = 64\n","\n","class VAE1(nn.Module):\n","    \"\"\"Variational Autoencoder with 1 hidden layer in the encoder and decoder.\"\"\"\n","    def __init__(self):\n","        super(VAE1, self).__init__()\n","\n","        # Encoder\n","        self.fc1 = nn.Linear(5, 256)\n","        self.fc2 = nn.Linear(256, latent_dim) # mu\n","        self.fc3 = nn.Linear(256, latent_dim) # logvar\n","\n","        # Decoder\n","        self.fc4 = nn.Linear(latent_dim, 256)\n","        self.fc5 = nn.Linear(256, 5)\n","\n","    def encoder(self, x):\n","        \"\"\"Encode the inputs to latent space.\"\"\"\n","        h1 = F.relu(self.fc1(x))\n","        return self.fc2(h1), self.fc3(h1)\n","\n","    def reparameterize(self, mu, logvar):\n","        \"\"\"Reparameterization trick to sample from the latent space.\"\"\"\n","        std = torch.exp(0.5*logvar) # calculate std\n","        eps = torch.randn_like(std) # random\n","        return mu + eps*std\n","\n","    def decoder(self, z):\n","        \"\"\"Decode the latent variables to the original space.\"\"\"\n","        h3 = F.relu(self.fc4(z))\n","        return torch.sigmoid(self.fc5(h3))\n","\n","    def forward(self, x):\n","        \"\"\"Forward pass through the network.\"\"\"\n","        mu, logvar = self.encoder(x.view(-1, 5))\n","        z = self.reparameterize(mu, logvar)\n","        return self.decoder(z), mu, logvar\n","\n","# Initialize the model\n","model = VAE1()\n","\n"],"metadata":{"id":"8QodKzZgawi2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a data loader\n","dataset = TensorDataset(torch.tensor(data))\n","data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n","\n","class VAE2(nn.Module):\n","    \"\"\"Variational Autoencoder with 2 hidden layers in the encoder and decoder.\"\"\"\n","    def __init__(self, input_dim=5, latent_dim=latent_dim):\n","        super(VAE2, self).__init__()\n","\n","        # Define the encoder\n","        self.encoder = nn.Sequential(\n","            nn.Linear(input_dim, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 32),\n","            nn.ReLU(),\n","            nn.Linear(32, latent_dim * 2)  # output a mean and a log variance\n","        )\n","\n","        # Define the decoder\n","        self.decoder = nn.Sequential(\n","            nn.Linear(latent_dim, 32),\n","            nn.ReLU(),\n","            nn.Linear(32, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, input_dim),\n","            nn.Sigmoid()  # output is between 0 and 1\n","        )\n","\n","    def reparameterize(self, mu, logvar):\n","        \"\"\"Reparameterization trick to sample from the latent space.\"\"\"\n","        std = torch.exp(0.5 * logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps * std\n","\n","    def forward(self, x):\n","        \"\"\"Forward pass through the network.\"\"\"\n","        mu_logvar = self.encoder(x.view(-1, 5)).view(-1, 2, latent_dim)\n","        mu = mu_logvar[:, 0, :]\n","        logvar = mu_logvar[:, 1, :]\n","        z = self.reparameterize(mu, logvar)\n","        return self.decoder(z), mu, logvar\n","\n","# Initialize the model\n","# model = VAE2()"],"metadata":{"id":"LJrD-ljvgHFu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 4: Train the Model"],"metadata":{"id":"YXNDrqhtbLmx"}},{"cell_type":"code","source":["\n","# Initialize the optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","EPOCHS = 100\n","\n","def loss_function(recon_x, x, mu, logvar, mse=True):\n","    \"\"\"Define the loss function: Reconstruction + KL divergence losses summed over all elements and batch.\"\"\"\n","    if mse:\n","        MSE = F.mse_loss(recon_x, x, reduction='sum')\n","        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","        return MSE + KLD\n","    else:\n","        BCE = F.binary_cross_entropy(recon_x, x.view(-1, 5), reduction='sum')\n","        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","        return BCE + KLD\n","\n","# Train the model\n","for epoch in range(EPOCHS):\n","    for batch in data_loader:\n","        batch = batch.to(model.fc1.weight.dtype)\n","        model.zero_grad()\n","        recon_batch, mu, logvar = model(batch)\n","        loss = loss_function(recon_batch, batch, mu, logvar)\n","        loss.backward()\n","        optimizer.step()\n","    print(f'Epoch {epoch}, Loss {loss.item()}')"],"metadata":{"id":"8wWhjcyja4Tn"},"execution_count":null,"outputs":[]}]}